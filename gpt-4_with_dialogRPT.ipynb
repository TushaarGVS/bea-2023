{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amasand/opt/miniconda3/envs/py3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# openai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = \"microsoft/DialogRPT-updown\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def score(ctx, hyp):\n",
    "  model_input = tokenizer.encode(ctx + hyp, return_tensors=\"pt\")\n",
    "  result = model(model_input, return_dict=True)\n",
    "  return torch.sigmoid(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt_with_retries(prompt_object, model = \"gpt-4-0314\", temperature = 1, max_tokens = 100, top_p = 1, frequency_penalty = 0, presence_penalty = 0, stop = None, logit_bias = {}, n = 1, retries = 20):\n",
    "    # this function queries gpt-3 with retries, because sometimes the api is down\n",
    "    # the function returns the response from gpt-4\n",
    "    # prompt_object is a list of dictionaries with role and content\n",
    "    # For example, prompt_object = [{\"role\": \"system\", \"content\": \"Hello\"}, {\"role\": \"user\", \"content\": \"Hi\"}, {\"role\":\"assistant\", \"content\": \"How can I help you?\"}]\n",
    "    \n",
    "    while retries > 0:\n",
    "        # print (\"Trying to query gpt-4 with retries = \", retries)\n",
    "        if retries == 10:\n",
    "            time.sleep(10)\n",
    "        try:\n",
    "            max_tokens = num_tokens_from_messages(prompt_object, model=model) + 150\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=prompt_object,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "                logit_bias=logit_bias,\n",
    "                stop=stop,\n",
    "                n=n\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            retries -= 1\n",
    "    return None \n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    elif model == \"gpt-4-0314\" or model == \"gpt-4\":\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += -1\n",
    "        num_tokens += 2\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jsonl \n",
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"train\": [],\n",
    "    \"dev\": [],\n",
    "}\n",
    "\n",
    "dataset_names = {\n",
    "    \"train\": \"train_with-reference\",\n",
    "    \"dev\": \"dev_without-reference\"\n",
    "}\n",
    "\n",
    "## sample prompt object\n",
    "\n",
    "# messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#         {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#     ]\n",
    "\n",
    "\n",
    "for split in dataset_names:\n",
    "    with open(os.path.join(\"data\", \"raw\", f\"{dataset_names[split]}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            data_line = json.loads(line)\n",
    "            sample = {\"context\": \"\", \"response\": \"\", \"dialogRPTcontext\": \"\", \"dialogRPTresponse\": \"\"}\n",
    "            utterances = data_line[\"utterances\"]\n",
    "            # [{'text': 'A) pull through', 'speaker': 'student'}, {'text': 'OK great', 'speaker': 'teacher'}, {'text': 'Not sure about the meaning of the second one... Does that person mean that being the prime minister he had to survive??', 'speaker': 'student'}] {'text': 'Ah yes good question - this is a bit ambiguous....', 'speaker': 'teacher'}\n",
    "            # sample[\"context\"] = \"\\n\".join([f\"{x['speaker']}: {x['text']}\" for x in utterances])\n",
    "            # make sample context into a prompt object\n",
    "            sample[\"context\"] = []\n",
    "            for i in range(len(utterances)):\n",
    "                if utterances[i]['speaker'] == 'student':\n",
    "                    # sample[\"context\"].append({\"role\": \"user\", \"content\": \"new utterance\"})\n",
    "                    sample[\"context\"].append({\"role\": \"user\", \"content\": utterances[i]['speaker'] + \": \" + utterances[i][\"text\"]})\n",
    "                else:\n",
    "                    sample[\"context\"].append({\"role\": \"assistant\", \"content\": utterances[i]['speaker'] + \": \" + utterances[i][\"text\"]})\n",
    "                    \n",
    "            dialogRPTcontext = ''\n",
    "            for dialogue_line in utterances:\n",
    "                dialogRPTcontext += \"'\" + dialogue_line['speaker'] + \"': \" + dialogue_line['text'] + ' <|endoftext|> '\n",
    "            sample[\"dialogRPTcontext\"] = dialogRPTcontext\n",
    "                        \n",
    "            \n",
    "            if \"response\" in data_line.keys():\n",
    "                response = data_line[\"response\"]\n",
    "                sample[\"response\"] = response['speaker'] + \": \" + response['text']\n",
    "                \n",
    "                dialogRPTresponse = \"'\" + response['speaker'] + \"': \" + response['text'] + ' <|endoftext|> ' \n",
    "                sample[\"dialogRPTresponse\"] = dialogRPTresponse \n",
    "\n",
    "            # print (sample)\n",
    "            data[split].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n",
      "{'context': [{'role': 'user', 'content': 'student: yes...maybe just is my internet problems, I can not stop it chang,sorry....'}, {'role': 'assistant', 'content': \"teacher: OK so here your big idea just needs to say whether you agree or not with the main idea....so can you tell me in just normal words if you do agree (don't worry about the essay answer - just in normal speaking if you like!)\"}], 'response': 'teacher: in other words: is exercise the best way?', 'dialogRPTcontext': \"'student': yes...maybe just is my internet problems, I can not stop it chang,sorry.... <|endoftext|> 'teacher': OK so here your big idea just needs to say whether you agree or not with the main idea....so can you tell me in just normal words if you do agree (don't worry about the essay answer - just in normal speaking if you like!) <|endoftext|> \", 'dialogRPTresponse': \"'teacher': in other words: is exercise the best way? <|endoftext|> \"}\n",
      "{'context': [{'role': 'assistant', 'content': 'teacher: In this one, can you change one word?'}, {'role': 'user', 'content': 'student: Facts'}, {'role': 'assistant', 'content': 'teacher: Ha, ok, that could work'}, {'role': 'assistant', 'content': \"teacher: or 'after that ___ fun fact about me'\"}, {'role': 'user', 'content': 'student: One fact'}, {'role': 'assistant', 'content': 'teacher: Ok, and if you use an article?'}, {'role': 'user', 'content': 'student: The'}, {'role': 'assistant', 'content': \"teacher: Hmm, the thing is, if you say 'the fun fact about me', it means there's only one fun fact about you. I'm sure that's not true! We are talking one of many fun facts, and the article is ???\"}, {'role': 'user', 'content': 'student: A'}], 'response': 'teacher: Yes!', 'dialogRPTcontext': \"'teacher': In this one, can you change one word? <|endoftext|> 'student': Facts <|endoftext|> 'teacher': Ha, ok, that could work <|endoftext|> 'teacher': or 'after that ___ fun fact about me' <|endoftext|> 'student': One fact <|endoftext|> 'teacher': Ok, and if you use an article? <|endoftext|> 'student': The <|endoftext|> 'teacher': Hmm, the thing is, if you say 'the fun fact about me', it means there's only one fun fact about you. I'm sure that's not true! We are talking one of many fun facts, and the article is ??? <|endoftext|> 'student': A <|endoftext|> \", 'dialogRPTresponse': \"'teacher': Yes! <|endoftext|> \"}\n",
      "{'context': [{'role': 'assistant', 'content': \"teacher: sorry number 4 should be: ...'until next month'\"}, {'role': 'user', 'content': 'student: 6 put back'}, {'role': 'assistant', 'content': \"teacher: Aahhhhh! no forget that...it's fine as it is\"}, {'role': 'assistant', 'content': 'teacher: Yes number 5 correct!'}, {'role': 'user', 'content': 'student: 1 put in?? can you install a kitchen??'}, {'role': 'assistant', 'content': 'teacher: yes absolutely (not in Spanish I guess!)'}, {'role': 'user', 'content': 'student: nooo'}, {'role': 'assistant', 'content': \"teacher: fit' would be common here to actually\"}], 'response': 'teacher: maybe a bit  more normal', 'dialogRPTcontext': \"'teacher': sorry number 4 should be: ...'until next month' <|endoftext|> 'student': 6 put back <|endoftext|> 'teacher': Aahhhhh! no forget that...it's fine as it is <|endoftext|> 'teacher': Yes number 5 correct! <|endoftext|> 'student': 1 put in?? can you install a kitchen?? <|endoftext|> 'teacher': yes absolutely (not in Spanish I guess!) <|endoftext|> 'student': nooo <|endoftext|> 'teacher': fit' would be common here to actually <|endoftext|> \", 'dialogRPTresponse': \"'teacher': maybe a bit  more normal <|endoftext|> \"}\n"
     ]
    }
   ],
   "source": [
    "# random_sample_idx = random.randint(0, len(data['train']))\n",
    "# print (random_sample_idx)\n",
    "# random_sample_idx = 1000\n",
    "# prompt_object = data['train'][random_sample_idx]['context']\n",
    "# num_few_shot_examples = 3\n",
    "# # randomly sample from data['train'] and append before prompt_object\n",
    "# for i in range(num_few_shot_examples):\n",
    "#     sample = random.choice(data['train'])\n",
    "#     print (sample)\n",
    "#     prompt_object = sample['context'] + [{'role': 'assistant', 'content': sample['response']}] + [{'role': 'user', 'content': 'new conversation'},] + prompt_object\n",
    "# prompt_object.insert(0, {\"role\": \"system\", \"content\": \"You are acting as a teacher, and you are helping a student learn, be patient, helpful and kind. Don't be super imposing, give short responses to encourage learning, make the student feel comfortable and confident and help them learn.\"})\n",
    "# response = data['train'][random_sample_idx]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached for default-gpt-4 in organization org-rocrupyvzgcl4yf25rqq6d1v on requests per min. Limit: 200 / min. Please try again in 300ms. Contact support@openai.com if you continue to have issues.\n",
      "'teacher': Hi <STUDENT>, ready when you are.... <|endoftext|> 'student': Hi <TEACHER>, I'm ready <|endoftext|> \n",
      "'teacher': Great! Let's start with a warm-up question. What is your favorite subject and why? |<endoftext|> 0.7912063598632812\n"
     ]
    }
   ],
   "source": [
    "gpt4_response = query_gpt_with_retries(prompt_object, model = \"gpt-4\")\n",
    "gpt4_response_str = gpt4_response.choices[0]['message']['content']\n",
    "\n",
    "dialogRPT_gpt4_response = gpt4_response_str.replace(\"teacher: \", \"'teacher': \") + \" |<endoftext|>\"\n",
    "print (data['train'][random_sample_idx]['dialogRPTcontext'])\n",
    "print (dialogRPT_gpt4_response, score(data['train'][random_sample_idx]['dialogRPTcontext'], dialogRPT_gpt4_response).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 examples\n",
      "Processed 10 examples\n",
      "Processed 20 examples\n",
      "Processed 30 examples\n",
      "Processed 40 examples\n",
      "Processed 50 examples\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 100 examples from train\n",
    "avg_score_gpt4 = 0.0\n",
    "avg_score_train = 0.0\n",
    "for i in range(100):\n",
    "    if i%10 == 0:\n",
    "        print (f\"Processed {i} examples\")\n",
    "    random_sample_idx = random.randint(0, len(data['train']))\n",
    "    random_sample_idx = 1000\n",
    "    prompt_object = data['train'][random_sample_idx]['context']\n",
    "    num_few_shot_examples = 3\n",
    "    # randomly sample from data['train'] and append before prompt_object\n",
    "    for i in range(num_few_shot_examples):\n",
    "        sample = random.choice(data['train'])\n",
    "        prompt_object = sample['context'] + [{'role': 'assistant', 'content': sample['response']}] + [{'role': 'user', 'content': 'new conversation'},] + prompt_object\n",
    "    prompt_object.insert(0, {\"role\": \"system\", \"content\": \"You are acting as a teacher, and you are helping a student learn, be patient, helpful and kind. Don't be super imposing, give short responses to encourage learning, make the student feel comfortable and confident and help them learn.\"})\n",
    "    response = data['train'][random_sample_idx]['response']\n",
    "    \n",
    "    gpt4_response = query_gpt_with_retries(prompt_object, model = \"gpt-4-0314\")\n",
    "    gpt4_response_str = gpt4_response.choices[0]['message']['content']\n",
    "\n",
    "    dialogRPT_gpt4_response = gpt4_response_str.replace(\"teacher: \", \"'teacher': \") + \" |<endoftext|>\"\n",
    "    # print (data['train'][random_sample_idx]['dialogRPTcontext'])\n",
    "    # print (dialogRPT_gpt4_response, score(data['train'][random_sample_idx]['dialogRPTcontext'], dialogRPT_gpt4_response).item())\n",
    "    dialogRPTscore_gpt4response = score(data['train'][random_sample_idx]['dialogRPTcontext'], dialogRPT_gpt4_response).item()\n",
    "    avg_score_gpt4 += dialogRPTscore_gpt4response\n",
    "    \n",
    "    dialogRPTscore_trainresponse = score(data['train'][random_sample_idx]['dialogRPTcontext'], data['train'][random_sample_idx]['dialogRPTresponse']).item()\n",
    "    avg_score_train += dialogRPTscore_trainresponse\n",
    "    \n",
    "print (\"The average dialogRPT score with gpt4 is: \", avg_score_gpt4/100)\n",
    "print (\"The average dialogRPT score with train is: \", avg_score_train/100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
