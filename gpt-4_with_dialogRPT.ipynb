{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amasand/opt/miniconda3/envs/py3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats\n",
    "from evaluate import load\n",
    "\n",
    "openai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "model = \"microsoft/DialogRPT-updown\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def score(ctx, hyp):\n",
    "  model_input = tokenizer.encode(ctx + hyp, return_tensors=\"pt\")\n",
    "  result = model(model_input, return_dict=True)\n",
    "  return torch.sigmoid(result.logits)\n",
    "\n",
    "\n",
    "def preprocess(preds, refs, num_ignore_chars=9):\n",
    "    return [pred[9:] for pred in preds], [ref[9:] for ref in refs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt_with_retries(prompt_object, model = \"gpt-4-0314\", temperature = 1, max_tokens = 100, top_p = 1, frequency_penalty = 0, presence_penalty = 0, stop = None, logit_bias = {}, n = 1, retries = 20):\n",
    "    # this function queries gpt-3 with retries, because sometimes the api is down\n",
    "    # the function returns the response from gpt-4\n",
    "    # prompt_object is a list of dictionaries with role and content\n",
    "    # For example, prompt_object = [{\"role\": \"system\", \"content\": \"Hello\"}, {\"role\": \"user\", \"content\": \"Hi\"}, {\"role\":\"assistant\", \"content\": \"How can I help you?\"}]\n",
    "    \n",
    "    while retries > 0:\n",
    "        # print (\"Trying to query gpt-4 with retries = \", retries)\n",
    "        if retries == 10:\n",
    "            time.sleep(10)\n",
    "        try:\n",
    "            max_tokens = num_tokens_from_messages(prompt_object, model=model) + 150\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=prompt_object,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty,\n",
    "                logit_bias=logit_bias,\n",
    "                stop=stop,\n",
    "                n=n\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            retries -= 1\n",
    "    return None \n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    elif model == \"gpt-4-0314\" or model == \"gpt-4\":\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += -1\n",
    "        num_tokens += 2\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jsonl \n",
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"train\": [],\n",
    "    \"dev\": [],\n",
    "}\n",
    "\n",
    "dataset_names = {\n",
    "    \"train\": \"train_with-reference\",\n",
    "    \"dev\": \"dev_without-reference\"\n",
    "}\n",
    "\n",
    "## sample prompt object\n",
    "\n",
    "# messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#         {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#     ]\n",
    "\n",
    "\n",
    "for split in dataset_names:\n",
    "    with open(os.path.join(\"data\", \"raw\", f\"{dataset_names[split]}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            data_line = json.loads(line)\n",
    "            sample = {\"context\": \"\", \"response\": \"\", \"dialogRPTcontext\": \"\", \"dialogRPTresponse\": \"\"}\n",
    "            utterances = data_line[\"utterances\"]\n",
    "            # [{'text': 'A) pull through', 'speaker': 'student'}, {'text': 'OK great', 'speaker': 'teacher'}, {'text': 'Not sure about the meaning of the second one... Does that person mean that being the prime minister he had to survive??', 'speaker': 'student'}] {'text': 'Ah yes good question - this is a bit ambiguous....', 'speaker': 'teacher'}\n",
    "            # sample[\"context\"] = \"\\n\".join([f\"{x['speaker']}: {x['text']}\" for x in utterances])\n",
    "            # make sample context into a prompt object\n",
    "            sample[\"context\"] = []\n",
    "            for i in range(len(utterances)):\n",
    "                if utterances[i]['speaker'] == 'student':\n",
    "                    # sample[\"context\"].append({\"role\": \"user\", \"content\": \"new utterance\"})\n",
    "                    sample[\"context\"].append({\"role\": \"user\", \"content\": utterances[i]['speaker'] + \": \" + utterances[i][\"text\"]})\n",
    "                else:\n",
    "                    sample[\"context\"].append({\"role\": \"assistant\", \"content\": utterances[i]['speaker'] + \": \" + utterances[i][\"text\"]})\n",
    "                    \n",
    "            dialogRPTcontext = ''\n",
    "            for dialogue_line in utterances:\n",
    "                dialogRPTcontext += \"'\" + dialogue_line['speaker'] + \"': \" + dialogue_line['text'] + ' <|endoftext|> '\n",
    "            sample[\"dialogRPTcontext\"] = dialogRPTcontext\n",
    "                        \n",
    "            \n",
    "            if \"response\" in data_line.keys():\n",
    "                response = data_line[\"response\"]\n",
    "                sample[\"response\"] = response['speaker'] + \": \" + response['text']\n",
    "                \n",
    "                dialogRPTresponse = \"'\" + response['speaker'] + \"': \" + response['text'] + ' <|endoftext|> ' \n",
    "                sample[\"dialogRPTresponse\"] = dialogRPTresponse \n",
    "\n",
    "            # print (sample)\n",
    "            data[split].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'assistant', 'content': \"teacher: Let's try again, you reconstruct the question\"}, {'role': 'user', 'content': 'student: Ok'}, {'role': 'assistant', 'content': \"teacher: I don't know whether you have ever watched Star Wars.\"}, {'role': 'user', 'content': 'student: Have you ever watched Star Wars?'}, {'role': 'assistant', 'content': 'teacher: Yes, well done!'}, {'role': 'assistant', 'content': 'teacher: I wonder if your lizard sleeps a lot.'}, {'role': 'user', 'content': 'student: Does your lizard sleeps a lot'}, {'role': 'assistant', 'content': 'teacher: Almost! Can you spot a mistake?'}, {'role': 'user', 'content': 'student: Sleep'}, {'role': 'assistant', 'content': 'teacher: Correct!'}, {'role': 'assistant', 'content': 'teacher: Ok, now could you change the question into reported speech, please'}, {'role': 'user', 'content': 'student: Õķ'}]\n",
      "teacher: Great! Let's start with the first one. 0.6589201092720032 0.8280081152915955\n",
      "[{'role': 'user', 'content': 'student: Ok'}, {'role': 'assistant', 'content': \"teacher: Because we don't have a question word, like 'how old' or 'where' or 'when'\"}, {'role': 'assistant', 'content': 'teacher: Makes sense?'}, {'role': 'user', 'content': 'student: Aha'}, {'role': 'assistant', 'content': \"teacher: Do you go to school? --- I don't know if you go to school. / I don't know whether you go to school.\"}, {'role': 'assistant', 'content': \"teacher: Let's try again, you reconstruct the question\"}, {'role': 'user', 'content': 'student: Ok'}, {'role': 'assistant', 'content': \"teacher: I don't know whether you have ever watched Star Wars.\"}, {'role': 'user', 'content': 'student: Have you ever watched Star Wars?'}]\n",
      "teacher: Perfect! Great job at reconstructing the question. Keep practicing and you'll continue to improve. 0.7341096997261047 0.831244170665741\n",
      "[{'role': 'assistant', 'content': \"teacher: Let's try again, you reconstruct the question\"}, {'role': 'user', 'content': 'student: Ok'}, {'role': 'assistant', 'content': \"teacher: I don't know whether you have ever watched Star Wars.\"}, {'role': 'user', 'content': 'student: Have you ever watched Star Wars?'}, {'role': 'assistant', 'content': 'teacher: Yes, well done!'}, {'role': 'assistant', 'content': 'teacher: I wonder if your lizard sleeps a lot.'}, {'role': 'user', 'content': 'student: Does your lizard sleeps a lot'}, {'role': 'assistant', 'content': 'teacher: Almost! Can you spot a mistake?'}, {'role': 'user', 'content': 'student: Sleep'}, {'role': 'assistant', 'content': 'teacher: Correct!'}, {'role': 'assistant', 'content': 'teacher: Ok, now could you change the question into reported speech, please'}, {'role': 'user', 'content': 'student: Õķ'}, {'role': 'assistant', 'content': \"teacher: Does it snow a lot in Ukraine? I don't know ...\"}, {'role': 'user', 'content': 'student: If it snows a lot in Ukraine'}]\n",
      "teacher: Excellent! You have the idea. Keep practicing this and you'll become a pro. 0.6417405605316162 0.8267578482627869\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 100 examples from train\n",
    "avg_dialogRPT_score_gpt4 = 0.0\n",
    "avg_dialogRPT_score_train = 0.0\n",
    "avg_bertscore_results = 0.0\n",
    "\n",
    "random_sample_idx_list = np.random.choice(len(data['train']), len(data['train']), replace=False)\n",
    "for i, random_sample_idx in enumerate(random_sample_idx_list):\n",
    "    # if i%10 == 0 and i != 0:\n",
    "    # print (f\"Processed {i} examples\")\n",
    "    # print (f\"The dialogRPT score at {i} examples with gpt4 is: \", avg_dialogRPT_score_gpt4/i)\n",
    "    # print (f\"The dialogRPT score at {i} examples  with train is: \", avg_dialogRPT_score_train/i)\n",
    "    # print (f\"The F1 bertscore at {i} examples is: \", avg_bertscore_results/i)\n",
    "\n",
    "    prompt_object = data['train'][random_sample_idx]['context']\n",
    "    \n",
    "    exists = False\n",
    "    for utterance in prompt_object:\n",
    "        if \"Let's try again, you reconstruct the question\" in utterance['content']:\n",
    "            exists = True  \n",
    "    if not exists:\n",
    "        continue\n",
    "    else:\n",
    "        print (prompt_object)\n",
    "    num_few_shot_examples = 3\n",
    "    # randomly sample from data['train'] and append before prompt_object\n",
    "    for i in range(num_few_shot_examples):\n",
    "        sample = random.choice(data['train'])\n",
    "        prompt_object = sample['context'] + [{'role': 'assistant', 'content': sample['response']}] + [{'role': 'user', 'content': 'new conversation'},] + prompt_object\n",
    "    prompt_object.insert(0, {\"role\": \"system\", \"content\": \"You are acting as a teacher, and you are helping a student learn, be patient, helpful and kind. Don't be super imposing, give short responses to encourage learning, make the student feel comfortable and confident and help them learn.\"})\n",
    "    response = data['train'][random_sample_idx]['response']\n",
    "    \n",
    "    gpt4_response = query_gpt_with_retries(prompt_object, model = \"gpt-4-0314\")\n",
    "    gpt4_response_str = gpt4_response.choices[0]['message']['content']\n",
    "\n",
    "    dialogRPT_gpt4_response = gpt4_response_str.replace(\"teacher: \", \"'teacher': \") + \" |<endoftext|>\"\n",
    "    # print (data['train'][random_sample_idx]['dialogRPTcontext'])\n",
    "    # print (dialogRPT_gpt4_response, score(data['train'][random_sample_idx]['dialogRPTcontext'], dialogRPT_gpt4_response).item())\n",
    "    dialogRPTscore_gpt4response = score(data['train'][random_sample_idx]['dialogRPTcontext'], dialogRPT_gpt4_response).item()\n",
    "    avg_dialogRPT_score_gpt4 += dialogRPTscore_gpt4response\n",
    "\n",
    "    dialogRPTscore_trainresponse = score(data['train'][random_sample_idx]['dialogRPTcontext'], data['train'][random_sample_idx]['dialogRPTresponse']).item()\n",
    "    avg_dialogRPT_score_train += dialogRPTscore_trainresponse\n",
    "    \n",
    "        \n",
    "    bertscore_gpt4response = bertscore.compute(predictions=[gpt4_response_str], references=[data['train'][random_sample_idx]['dialogRPTresponse']], lang='en',\n",
    "                            model_type='roberta-large')['f1'][0]\n",
    "    avg_bertscore_results += bertscore_gpt4response\n",
    "    \n",
    "# print (\"The average dialogRPT score with gpt4 is: \", avg_dialogRPT_score_gpt4/100)\n",
    "# print (\"The average dialogRPT score with train is: \", avg_dialogRPT_score_train/100)\n",
    "# print (\"The average F1 bertscore is: \", avg_bertscore_results/100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
